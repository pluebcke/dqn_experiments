{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling DQN\n",
    "Dueling DQN tries to improve on DQN by splitting the neural network into two parts. The first part predicts the state value $V(s)$ while the second part predicts the advantage of using a specific action in the current state $A(s,a)$ [1]. The Q-value can then be obtained by adding the advantage to the state value:\n",
    "\n",
    "$$Q(s,a) = V(s) + A(s,a)$$\n",
    "(even though there are a few more details in the actual implementation). Implemented this way, the state value can be learned more accurately, since it is updated more frequently [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Just a very brief overview of the results: implementing dueling networks did not lead to a major improvement in the [BSuite](https://github.com/deepmind/bsuite) experiments. Some experiments have slightly higher scores while others are a bit worse with dueling networks.  \n",
    "\n",
    "My assumption is that the agent at this point is already quite good for the basic scale and noise experiments. The mnist experiments are one exception, however, as I added to the basiq DQN results, the baseline [DQN agent](https://github.com/deepmind/bsuite/tree/master/bsuite/baselines/tf/dqn) did show similarly bad results on the mnist task (different from the BSuite publication [2]). So maybe there is an issue with the experiment itself at the moment.\n",
    "\n",
    "Dueling DQN is supposed to improve the q-values, but I don't see any reason to expect it can improve exploration, credit assignment or memory. Therefore these results are not very suprising.\n",
    "Maybe one idea for the future would be adapting this agent for completely different experiments to further investigate the influence of the different improvements.\n",
    "\n",
    "![Results of an agent with all previously discussed improvements. Without (run018) and with (run019) duelling neural networks.](./figures/duel_dqn_barplot.png)\n",
    "\n",
    "The barplot was again created with the analysis Jupyter Notebook from the BSuite repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Wang, Ziyu, et al. Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581, 2015.  \n",
    "[2] Osband, Ian, et al. \"Behaviour Suite for Reinforcement Learning.\" arXiv preprint arXiv:1908.03568, 2019. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
