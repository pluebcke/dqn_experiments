{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-step returns\n",
    "The next improvement from the Rainbow manuscript [1] which I implemented are n-step returns. \n",
    "A good and thorough introduction to n-step returns can be found in Chapter 7 of the Barto & Sutton RL book [2].\n",
    "The idea is to replace the one step return and next state's Q-value with the discounted return over n-steps and use Q-value of the n-th state for the target of the update step:\n",
    "\n",
    "$$ y_t = r_{t+1} +... + \\gamma^{n-1} r_{t+n} + \\gamma^{n} \\cdot Q(s_{t+n}, \\text{argmax}_{a} Q(s_{t+n}, a, \\Theta), \\Theta^{-})$$\n",
    "\n",
    "Using n-step rewards has two advantages: first, it can help speed up the learning of q-values. Second, the n-step update reduces the bias of the method (at the cost of an increasing variance). Even though I am almost certain that Barto & Sutton discuss the bias/variance trade-off of n-step methods, I was not able to find it after briefly looking through the cited book, and a web search revealed [3], which has an excellent discussion of the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "I implemented n-steps return by adding a second deque as a buffer to the ReplayMemory. This buffer stores up to n samples and once it is full it starts filling the main ReplayMemory with samples that contain the n-step reward.\n",
    "All figures were again plotted with help of the BSuite analysis Jupyter Notebook [4].\n",
    "\n",
    "The comparison of the DDQN agent without 1-step returns (run014) and 4-step returns (run016) is shown below. The agent with 4-step returns shows slightly better results. Further increasing n up to 8 did not lead to better performance. \n",
    "\n",
    "**Edit:** The original results of run016 had DDQN switched off. Run18 shows results with 4-step returns and DDQN switched on. While the results are slightly different, there is no big difference.\n",
    "\n",
    "![Radar plot with a comparison between the different settings.](./figures/n_step_radar.png)\n",
    "\n",
    "The bar plot shows that the main performance increase comes from the catch, cartpole and  mountain car experiments and their noisy / scaled variants. Not too suprisingly n-step rewards had no impact on the bandit experiment (since it ends after one step, only 1-step returns are used in this experiment) or on exploration and memory. \n",
    "\n",
    "![Bar plot with a comparison between the different settings.](./figures/n_step_barplot.png)\n",
    "\n",
    "As one example, the learning curves for the cartpole experiment show that using 4-step rewards largely increases and stabilizies the performance.\n",
    "\n",
    "![Learning curves for the cartpole experiment. DQN with 1-step rewards (run014); DQN with 4-step rewards (run016) and DDQN with 4-step rewards](./figures/n_step_cartpole.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Hessel, Matteo, et al. Rainbow: Combining improvements in deep reinforcement learning. In: Thirty-Second AAAI Conference on Artificial Intelligence. 2018.  \n",
    "[2] Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.  \n",
    "[3] https://www.endtoend.ai/blog/bias-variance-tradeoff-in-reinforcement-learning/, last accessed: 2020-03-28.  \n",
    "[4] Osband, Ian, et al. \"Behaviour Suite for Reinforcement Learning.\" arXiv preprint arXiv:1908.03568, 2019.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
